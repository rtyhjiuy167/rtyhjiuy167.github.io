---
title: 17-3 随机森林分类
top: 17.3
tags:
  - python
categories:
  - python
---

<h3>随机森林分类</h3>

单个决策树的准确率越高，随机森林的准确率也会越高

随机森林的本质是一种装袋集成算法（bagging），装袋集成算法是对基评估器的预测结果进行平均或用多数表决原则来决定集成评估器的结果

决策树从最重要的特征中随机选择出一个特征来进行分枝，因此每次生成的决策树都不一样，这个随机模式由参数random_state控制

1.

```python
# 使用红酒数据集，对随机森林和单棵决策树进行比较
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

wine = load_wine()  # 不是.csv 没有info() head()的方法

# 红酒数据里的 特证矩阵 标签 是分开的
Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3)

# 1.实例化
clf = DecisionTreeClassifier(random_state=0)
rfc = RandomForestClassifier(random_state=0)
# 2.训练模型
clf = clf.fit(Xtrain, Ytrain)
rfc = rfc.fit(Xtrain, Ytrain)

# 单棵决策树（分类树） 与 随机森林
score_c = clf.score(Xtest, Ytest)
score_r = rfc.score(Xtest, Ytest)

print("Single Tree:{} Random Forest:{}".format(score_c, score_r))  # 结果是随机森林较高
```

2.

```python
# 使用红酒数据集，对随机森林和决策树进行比较 一组交叉验证
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

wine = load_wine()

clf = DecisionTreeClassifier(random_state=25)
clf_s = cross_val_score(clf, wine.data, wine.target, cv=10)

rfc = RandomForestClassifier(random_state=25)
rfc_s = cross_val_score(rfc, wine.data, wine.target, cv=10)

plt.plot(range(1, 11), rfc_s, label="RandomForest")
plt.plot(range(1, 11), clf_s, label="DecisionTree")
plt.legend()
plt.show()
# 明显 随机森林更稳定
```

3.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

wine = load_wine()
rfc_l = []
clf_l = []
for i in range(10):
    rfc = RandomForestClassifier(n_estimators=25)  # 25棵树
    rfc_s = cross_val_score(rfc, wine.data, wine.target, cv=10).mean()
    rfc_l.append(rfc_s)
    clf = DecisionTreeClassifier()
    clf_s = cross_val_score(clf, wine.data, wine.target, cv=10).mean()
    clf_l.append(clf_s)

plt.plot(range(1, 11), rfc_l, label="Random Forest")
plt.plot(range(1, 11), clf_l, label="Decision Tree")
plt.legend()
plt.show()
```

```python
import numpy as np
from scipy.special import comb

# 假设一棵树判断错误的可能性为0.2
# 因为对任何一个样本而言，平均或多数表决原则下，当且仅当有半数以上的树判断错误，随机森林才会判断错误
# 以25棵树为例，概率如下：
print(np.array([comb(25, i) * (0.2 ** i) * ((1 - 0.2) ** (25 - i)) for i in range(13, 26)]).sum())
```

<h5>estimators 查看森林中树的状况</h5>

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

wine = load_wine()

Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3)

rfc = RandomForestClassifier(n_estimators=20, random_state=2)
rfc = rfc.fit(Xtrain, Ytrain)

# 随机森林的重要属性之一：estimators，查看森林中树的状况
print(rfc.estimators_)
for i in range(len(rfc.estimators_)):
    print(rfc.estimators_[i])

for i in range(len(rfc.estimators_)):
    print(rfc.estimators_[i].random_state)
```

<h5>n_estimators的学习曲线</h5>

n_estimators 这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

wine = load_wine()

# n_estimators的学习曲线
superpa = []
for i in range(200):  # 时间要几分钟
    rfc = RandomForestClassifier(n_estimators=i + 1, n_jobs=-1)
    rfc_s = cross_val_score(rfc, wine.data, wine.target, cv=10).mean()
    superpa.append(rfc_s)

# 打印最高准确率
print(max(superpa), superpa.index(max(superpa)))
plt.figure(figsize=[20, 5])
plt.plot(range(1, 201), superpa)
plt.show()
```

<h5> bootstrap & oob_score</h5>

bootstrap参数默认True，代表采用有放回的随机抽样技术。通常，这个参数不会被我们设置为False

在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一 样大的，n个样本组成的自助集

有放回抽样可能会造成一些样本在同一个自助集中出现多次，而其他一些却可能被忽略，一般来说，自助集大约平均会包含63%(极限情况)的原始数据。因此，会有约37%的训练数据被浪费掉，没有参与建模， 这些数据被称为袋外数据(out of bag data，简写为oob)。

在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可

当样本数和n_estimators都不够大的时候，很可能就没有数据掉落在袋外，自然也就无法使用oob数据来测试模型了

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine

wine = load_wine()

# 用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True
rfc = RandomForestClassifier(n_estimators=25, oob_score=True)
rfc = rfc.fit(wine.data, wine.target)

# 训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果
print(rfc.oob_score_)

```

<h5>基分类器的误差率和随机森林的误差率之间的图像</h5>

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import comb

x = np.linspace(0, 1, 20)
y = []
for epsilon in np.linspace(0, 1, 20):
    E = np.array([comb(25, i) * (epsilon ** i) * ((1 - epsilon) ** (25 - i))
                  for i in range(13, 26)]).sum()
    y.append(E)
plt.plot(x, y, "o-", label="when estimators are different")
plt.plot(x, x, "--", color="red", label="if all estimators are same")
plt.xlabel("individual estimator's error")
plt.ylabel("RandomForest's error")
plt.legend()
plt.show()
```

可以从图像上看出，当基分类器（单棵树）的误差率小于0.5，即准确率大于0.5时，集成（森林）的效果是比基分类器要好的。相反， 当基分类器的误差率大于0.5，袋装的集成算法就失效了。所以在使用随机森林之前，一定要检查，用来组成随机森林的分类树们是否都有至少50%的预测正确率

<h4>随机森林在乳腺癌数据的调参</h4>

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

breast_cancer = load_breast_cancer()

print(breast_cancer.data.shape)  # (569, 30) # 可看出数据量小 数据量与特征比也很小
rfc = RandomForestClassifier(n_estimators=100, random_state=90)
score_pre = cross_val_score(rfc, breast_cancer.data, breast_cancer.target, cv=10).mean()
print(score_pre)

scorel = []
for i in range(1, 200, 1):
    rfc = RandomForestClassifier(n_estimators=i,
                                 random_state=90)
    score = cross_val_score(rfc, breast_cancer.data, breast_cancer.target, cv=10).mean()
    scorel.append(score)

print(max(scorel), (scorel.index(max(scorel)) * 10) + 1)
plt.figure(figsize=[20, 5])
plt.plot(range(1, 200, 1), scorel)
plt.show()
```

<h4>泰坦尼克号 随机森林</h4>

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

data = pd.read_csv("data.csv")

# 筛选特征
data.drop(['Cabin', 'Name', 'Ticket'], inplace=True, axis=1)

# 处理缺失值
data.loc[:, "Age"] = data.loc[:, "Age"].fillna(data.loc[:, "Age"].mean())
data = data.dropna()  # 删除有缺失数据

data.loc[:, "Sex"] = (data.loc[:, "Sex"] == "male").astype("int")

labels = data.loc[:, "Embarked"].unique().tolist()
data.loc[:, "Embarked"] = data.loc[:, "Embarked"].apply(lambda x: labels.index(x))

x = data.iloc[:, data.columns != "Survived"]
y = data.iloc[:, data.columns == "Survived"]

print(type(y))  # <class 'pandas.core.frame.DataFrame'>
print(type(x))  # <class 'pandas.core.frame.DataFrame'>
print(type(y.values.ravel()))  # <class 'numpy.ndarray'>

rfc = RandomForestClassifier(n_estimators=68,
                             random_state=90,
                             criterion="gini",
                             min_samples_split=8,
                             min_samples_leaf=1,
                             max_depth=12,
                             max_features=2,
                             max_leaf_nodes=36)
rfc_s = cross_val_score(rfc, x, y.values.ravel(), cv=10).mean()
# cross_val_score 交叉验证 传的y应是numpy.ndarray x可都可

print(rfc_s)
# 0.838023493360572
```
