---
title: 17-6 特征过滤
top: 17.6
tags:
  - python
categories:
  - python
---

<h4>方差过滤</h4>

若一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征。

```python
import pandas as pd
from sklearn.feature_selection import VarianceThreshold  # 这是通过特征本身的方差来筛选特征的类
import numpy as np

data = pd.read_csv("digit recognizor.csv")
X = data.iloc[:, 1:]
y = data.iloc[:, 0]
print(data.head())
#    label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783
# 0      1       0       0       0  ...         0         0         0         0
# 1      0       0       0       0  ...         0         0         0         0
# 2      1       0       0       0  ...         0         0         0         0
# 3      4       0       0       0  ...         0         0         0         0
# 4      0       0       0       0  ...         0         0         0         0

print(X.shape)  # (42000, 784)
"""
这个数据量相对夸张，如果使用支持向量机和神经网络，很可能会直接跑不出来。使用KNN跑一次大概需要半个小时。
用这个数据举例，能更够体现特征工程的重要性。
"""

selector = VarianceThreshold()  # 实例化，不填参数默认方差为0
X_var0 = selector.fit_transform(X)  # 获取删除不合格特征之后的新特征矩阵
print(X_var0.shape)  # (42000, 708)

# .var() 用来读取方差
# np.median(X.var().values) 选取方差的中位数 方差在这以下的特征全部不要
X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
print(X_fsvar.shape)  # (42000, 392)

# 当特征是二分类时，特征的取值就是伯努利随机变量 .var() = p(1-p)
X_bvar = VarianceThreshold(.8 * (1 - .8)).fit_transform(X)
print(X_bvar.shape)  # (42000, 685)

```

最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进 行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。<br>对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平

在sklearn中，决策树和随机森林都是随机选择特征进行分枝，但决策树在建模过程中随机抽取的特征数目却远远超过随机森林当中每棵树随机抽取 的特征数目（比如说对于这个780维的数据，随机森林每棵树只会抽取10~20个特征，而决策树可能会抽取 300~400个特征），因此，过滤法对随机森林无用，却对决策树有用。

过滤法的**主要目的**是：在维持算法表现的前提下，帮助算法们降低计算成本

```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv("digit recognizor.csv")
X = data.iloc[:, 1:]
y = data.iloc[:, 0]

# 过滤前 使用随机森林 耗时几分钟
res = cross_val_score(RandomForestClassifier(n_estimators=300, random_state=0), X, y, cv=5).mean()
print(res)  # 0.9653095238095238

# 最近邻算法KNN 耗时过长3小时以上，不建议用
# print(cross_val_score(KNN(),X,y,cv=5).mean())

```

```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import VarianceThreshold

data = pd.read_csv("digit recognizor.csv")
X = data.iloc[:, 1:]
y = data.iloc[:, 0]

selector = VarianceThreshold()
X_var0 = selector.fit_transform(X)

# 排除过滤后 使用随机森林 耗时几分钟
res = cross_val_score(RandomForestClassifier(n_estimators=300, random_state=0), X_var0, y, cv=5).mean()
print(res)  # 0.965452380952381
```