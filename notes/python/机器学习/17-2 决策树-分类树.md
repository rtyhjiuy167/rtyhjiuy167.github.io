---
title: 17-2 决策树-分类树
top: 17.2
tags:
  - python
categories:
  - python
---

<h3>分类树</h3>

<h5>先看看使用的数据集</h5>

```python
# datasets 库里有很多数据集
# 这里导入红酒数据
from sklearn.datasets import load_wine
import pandas as pd
wine = load_wine()
# wine 是一个字典 里面有 data target(标签) feature_names(数据的名字) target_names(标签名) 等key
print(wine.target)
# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]

print(wine.feature_names)
# ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash',
# 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols',
# 'proanthocyanins', 'color_intensity', 'hue',
# 'od280/od315_of_diluted_wines', 'proline']

print(wine.target_names)  # ['class_0' 'class_1' 'class_2']

print(pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1))
#         0     1     2     3      4     5   ...    8      9     10    11      12  0
# 0    14.23  1.71  2.43  15.6  127.0  2.80  ...  2.29   5.64  1.04  3.92  1065.0   0
# 1    13.20  1.78  2.14  11.2  100.0  2.65  ...  1.28   4.38  1.05  3.40  1050.0   0
# 2    13.16  2.36  2.67  18.6  101.0  2.80  ...  2.81   5.68  1.03  3.17  1185.0   0
# 3    14.37  1.95  2.50  16.8  113.0  3.85  ...  2.18   7.80  0.86  3.45  1480.0   0
# 4    13.24  2.59  2.87  21.0  118.0  2.80  ...  1.82   4.32  1.04  2.93   735.0   0
# ..     ...   ...   ...   ...    ...   ...  ...   ...    ...   ...   ...     ...  ..
# 173  13.71  5.65  2.45  20.5   95.0  1.68  ...  1.06   7.70  0.64  1.74   740.0   2
# 174  13.40  3.91  2.48  23.0  102.0  1.80  ...  1.41   7.30  0.70  1.56   750.0   2
# 175  13.27  4.28  2.26  20.0  120.0  1.59  ...  1.35  10.20  0.59  1.56   835.0   2
# 176  13.17  2.59  2.37  20.0  120.0  1.65  ...  1.46   9.30  0.60  1.62   840.0   2
# 177  14.13  4.10  2.74  24.5   96.0  2.05  ...  1.35   9.20  0.61  1.60   560.0   2
#
# [178 rows x 14 columns]

# 这个数据集只有178行，14列。数据量还是很小的。最后一列是我们的标签，每个数字对应一个具体的分类
```

<h5>模型的简单使用</h5>

```python
from sklearn import tree

import graphviz

from sklearn.datasets import load_wine

from sklearn.model_selection import train_test_split  # 导入用来训练和测试集的类

wine = load_wine()

# 将数据分为训练集和测试集 train_test_split(数据特征，标签，test_size测试比例(剩余为训练集))
Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3)

# sklearn的基本建模流程
# 1.实例化，建立评估模型对象
clf = tree.DecisionTreeClassifier()

# 2.通过模型接口训练模型
clf = clf.fit(Xtrain, Ytrain)

# 3.通过模型接口提取需要的信息
#  返回预测的准确度
score_train = clf.score(Xtrain, Ytrain)
score_test = clf.score(Xtest, Ytest)
print(score_train)  # 训练集的准确度
print(score_test)  # 测试集的准确度

# 导入训练好的模型 替换标签名 数据名
dot_data = tree.export_graphviz(clf, feature_names=wine.feature_names, class_names=wine.target_names)

# 导出树
graph = graphviz.Source(dot_data)

# 保存source到文件，并提供Graphviz引擎
graph.render('./graph.gv', view=True)
```

<h5>设置参数</h5>

```python
from sklearn import tree

import graphviz

from sklearn.datasets import load_wine

from sklearn.model_selection import train_test_split

wine = load_wine()

Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3)

clf = tree.DecisionTreeClassifier(criterion="entropy", random_state=30, splitter="random", min_samples_split=10,
                                  min_samples_leaf=1, max_depth=3)

clf = clf.fit(Xtrain, Ytrain)

score_test = clf.score(Xtest, Ytest)
score_train = clf.score(Xtrain, Ytrain)

feature_name = ['酒精', '苹果酸', '灰', '灰的碱性', '镁', '总酚', '类黄酮', '非黄烷类酚类', '花青素', '颜色强度', '色调', 'od280/od315稀释葡萄酒', '脯氨酸']
class_name = ["琴酒", "雪莉", "贝尔摩德"]

# 参数 fill 颜色填充
# 参数 rounded 框的形状为圆角
# feature_names=feature_name 将特征的名字用中文代替
# class_names=class_name 将标签的名字用中文代替
dot_data = tree.export_graphviz(clf, feature_names=feature_name, class_names=class_name, filled=True,
                                rounded=True, fontname="MicroSoft YaHei")
# 导出树
graph = graphviz.Source(dot_data)

# 保存source到文件，并提供Graphviz引擎
graph.render('./graph.gv', view=True)

# clf.feature_importances_ 特征的重要性 贡献度
print(list(zip(feature_name, clf.feature_importances_)))

# 当训练集的准确率>测试集的准确率 可以认为其过拟合
print(score_train)
print(score_test)
```

<h3>泰坦尼克号生存者预测</h3>

<h5>导入数据集，探索数据</h5>

```python
import pandas as pd

data = pd.read_csv("data.csv")

# 查看表的信息 重要！！
print(data.info()) 
# <class 'pandas.core.frame.DataFrame'>
# RangeIndex: 891 entries, 0 to 890
# Data columns (total 12 columns):
#  #   Column       Non-Null Count  Dtype
# ---  ------       --------------  -----
#  0   PassengerId  891 non-null    int64
#  1   Survived     891 non-null    int64
#  2   Pclass       891 non-null    int64
#  3   Name         891 non-null    object
#  4   Sex          891 non-null    object
#  5   Age          714 non-null    float64
#  6   SibSp        891 non-null    int64
#  7   Parch        891 non-null    int64
#  8   Ticket       891 non-null    object
#  9   Fare         891 non-null    float64
#  10  Cabin        204 non-null    object
#  11  Embarked     889 non-null    object
# dtypes: float64(2), int64(5), object(5)
# memory usage: 83.7+ KB


# 查看表的前n行信息 重要！！
print(data.head(20))
#     PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked
# 0             1         0       3  ...   7.2500   NaN         S
# 1             2         1       1  ...  71.2833   C85         C
# 2             3         1       3  ...   7.9250   NaN         S
# 3             4         1       1  ...  53.1000  C123         S
# 4             5         0       3  ...   8.0500   NaN         S
# 5             6         0       3  ...   8.4583   NaN         Q
# 6             7         0       1  ...  51.8625   E46         S
# 7             8         0       3  ...  21.0750   NaN         S
# 8             9         1       3  ...  11.1333   NaN         S
# 9            10         1       2  ...  30.0708   NaN         C
# 10           11         1       3  ...  16.7000    G6         S
# 11           12         1       1  ...  26.5500  C103         S
# 12           13         0       3  ...   8.0500   NaN         S
# 13           14         0       3  ...  31.2750   NaN         S
# 14           15         0       3  ...   7.8542   NaN         S
# 15           16         1       2  ...  16.0000   NaN         S
# 16           17         0       3  ...  29.1250   NaN         Q
# 17           18         1       2  ...  13.0000   NaN         S
# 18           19         0       3  ...  18.0000   NaN         S
# 19           20         1       3  ...   7.2250   NaN         C

# [20 rows x 12 columns]

"""
可看出
	cabin的缺失值过多，有891条数据，只有204条数据有该值，之后不予填充，应删掉该特征
	Name与Ticket与存活没有相关性，也应去除
	Age与存活具有重要相关性，缺失值不是很大，应予填充
		在年龄上可以用平均值来填补，但因为与实际不是很相符，所以对次填补相当于添加了噪声
	Embarked有缺失值，但过少，应直接将缺少的那两条数据直接删除
	
	Embarked和Sex是分类变量应转换为数值型变量（因为决策树不能处理文字）
"""
```

<h5>对数据集进行预处理</h5>

```python
import pandas as pd
from sklearn.model_selection import train_test_split

data = pd.read_csv("data.csv")

# 筛选特征
# inplace 覆盖原表 axis=1 删除所在列
data.drop(['Cabin', 'Name', 'Ticket'], inplace=True, axis=1)

# 处理缺失值
# data.loc 文字索引(即只能传标签名) 例如data.loc[:, "Age"]  对所有的行，带有 "Age"的列 都取出来
# 非文字索引 使用 data.iloc
data.loc[:, "Age"] = data.loc[:, "Age"].fillna(data.loc[:, "Age"].mean())  # 用平均值来年龄
data = data.dropna()  # 只要某条数据的特征有缺失，直接删除该条数据

# unique() 用来查看有多少类（重复值去掉），得到类的数组
labels = data.loc[:, "Embarked"].unique().tolist()
# data["Embarked"] 得到Embarked所在列 apply 对该列执行操作 x得到是每个Embarked的值  根据值去替换labels列表中的索引
data.loc[:, "Embarked"] = data.loc[:, "Embarked"].apply(lambda x: labels.index(x))

# 将性别转成对应的1,0 并调整类型为 int
data.loc[:, "Sex"] = (data.loc[:, "Sex"] == "male").astype("int")

#
x = data.iloc[:, data.columns != "Survived"]
y = data.iloc[:, data.columns == "Survived"]

Xtrain, Xtext, Ytrain, Ytext = train_test_split(x, y, test_size=0.3)

# 因为是随机选取训练集和测试集，数据自带的索引已经乱序了
print(Xtrain.head(10))
#      PassengerId  Pclass  Sex        Age  SibSp  Parch      Fare  Embarked
# 795          796       2    1  39.000000      0      0   13.0000         0
# 33            34       2    1  66.000000      0      0   10.5000         0
# 5              6       3    1  29.699118      0      0    8.4583         2
# 60            61       3    1  22.000000      0      0    7.2292         1
# 179          180       3    1  36.000000      0      0    0.0000         0
# 31            32       1    0  29.699118      1      0  146.5208         1
# 830          831       3    0  15.000000      1      0   14.4542         1
# 515          516       1    1  47.000000      0      0   34.0208         0
# 256          257       1    0  29.699118      0      0   79.2000         1
# 269          270       1    0  35.000000      0      0  135.6333         0

print(Xtrain.index)
# Int64Index([373,  77, 349, 767, 716, 325, 182, 137, 234, 501,
#             ...
#             751, 485, 378, 116, 558, 326, 792, 200, 464,  44],
#            dtype='int64', length=622)

#  在没有刻意要保持乱序的前提下，应纠正索引
# 循环4次，将每个列表的索引数组进行纠正索引
for i in [Xtrain, Xtext, Ytrain, Ytext]:
    i.index = range(i.shape[0])
```

<h5>完整代码</h5>

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

data = pd.read_csv("data.csv")

# 筛选特征
data.drop(['Cabin', 'Name', 'Ticket'], inplace=True, axis=1)

# 处理缺失值
data.loc[:, "Age"] = data.loc[:, "Age"].fillna(data.loc[:, "Age"].mean())
data = data.dropna()  # 删除有缺失数据

data.loc[:, "Sex"] = (data.loc[:, "Sex"] == "male").astype("int")

labels = data.loc[:, "Embarked"].unique().tolist()
data.loc[:, "Embarked"] = data.loc[:, "Embarked"].apply(lambda x: labels.index(x))

x = data.iloc[:, data.columns != "Survived"]
y = data.iloc[:, data.columns == "Survived"]

Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.3)

# 纠正索引
for i in [Xtrain, Xtest, Ytrain, Ytest]:
    i.index = range(i.shape[0])

# 学习曲线
tr = []
te = []
for i in range(10):
    clf = DecisionTreeClassifier(random_state=25, max_depth=i + 1, criterion="entropy")
    clf = clf.fit(Xtrain, Ytrain)
    score_tr = clf.score(Xtrain, Ytrain)  # 训练拟合分数
    score_te = cross_val_score(clf, x, y, cv=10).mean()  # 这里用的是交叉验证，不用将数据分开。返回测试拟合的分数
    tr.append(score_tr)
    te.append(score_te)
print(max(te))
plt.plot(range(1, 11), tr, color="red", label="train")
plt.plot(range(1, 11), te, color="blue", label="test")
plt.xticks(range(1, 11))
plt.legend()
plt.show()
```

<h5>网格搜索的完整代码</h5>

```python
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

data = pd.read_csv("data.csv")

# 筛选特征
data.drop(['Cabin', 'Name', 'Ticket'], inplace=True, axis=1)

# 处理缺失值
data.loc[:, "Age"] = data.loc[:, "Age"].fillna(data.loc[:, "Age"].mean())
data = data.dropna()  # 删除有缺失数据

data.loc[:, "Sex"] = (data.loc[:, "Sex"] == "male").astype("int")

labels = data.loc[:, "Embarked"].unique().tolist()
data.loc[:, "Embarked"] = data.loc[:, "Embarked"].apply(lambda x: labels.index(x))

x = data.iloc[:, data.columns != "Survived"]
y = data.iloc[:, data.columns == "Survived"]

Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.3)

# 纠正索引
for i in [Xtrain, Xtest, Ytrain, Ytest]:
    i.index = range(i.shape[0])



# 生成 0~0.5 里有顺序的随机数
gini_threholds = np.linspace(0, 0.5, 50) # 基尼系数的范围本就是 0~0.5 但也常用来作为其它的参考
# entropy_threholds =np.linspace(0, 1, 50) # entropy的范围本就是 0~1

clf = DecisionTreeClassifier(random_state=25)
# 网格搜索 能够同时调整多个参数 用的是枚举 计算量较大（最好先确定范围）
# 网格搜索不一定有我们自己调的参数好，因为在网格搜索中，设置的参数是一定要用上的
# parameters 本质是一串参数，以及这些参数对应的我们希望用网格搜索来搜索的参数的取值范围
parameters = {"criterion": ("gini", "entropy"),
              "splitter": ("best", "random"),
              "max_depth": [*range(1, 10)],
              "min_samples_leaf": [*range(1, 50, 5)],
              "min_impurity_decrease": [*np.linspace(0, 0.5, 50)]
              }
GS = GridSearchCV(clf, parameters, cv=10)
Gs = GS.fit(Xtrain, Ytrain)
# 从输入的参数和参数取值列表中，返回最佳组合
print(GS.best_params_)
# {'criterion': 'gini', 'max_depth': 7, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 6, 'splitter': 'best'}

# 网格搜索后的评判标准
print(GS.best_score_)
# 0.8217357910906298
```

